{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vepnfz2ZHten",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self,embeddingSize, headsCount):\n",
        "    super(Attention, self).__init__()\n",
        "    self.embeddingSize = embeddingSize\n",
        "    self.headsCount = headsCount\n",
        "\n",
        "    assert (embeddingSize%headsCount == 0), \"Embedding Size need to be divisible by the count of heads\"\n",
        "    \n",
        "    self.headDim = embeddingSize // headsCount\n",
        "    self.V = nn.Linear(self.headDim, self.headDim, bias = False)\n",
        "    self.K = nn.Linear(self.headDim, self.headDim, bias = False)\n",
        "    self.Q = nn.Linear(self.headDim, self.headDim, bias = False)\n",
        "    self.linear = nn.Linear(embeddingSize, embeddingSize)\n",
        "  \n",
        "  def forward(self, V, K, Q, mask):\n",
        "    \n",
        "    V = V.reshape(V.shape[0], V.shape[1], self.headsCount, self.headDim)\n",
        "    K = K.reshape(K.shape[0], K.shape[1], self.headsCount, self.headDim)\n",
        "    Q = Q.reshape(Q.shape[0], Q.shape[1], self.headsCount, self.headDim)\n",
        "\n",
        "    V = self.V(V)\n",
        "    K = self.K(K)\n",
        "    Q = self.Q(Q)\n",
        "\n",
        "    energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    \n",
        "    attention = torch.softmax(energy/(self.embeddingSize ** 1/2), dim = 3)\n",
        "    \n",
        "    valueWeights = torch.einsum(\"nhqk,nkhd->nqhd\",[attention, V])\n",
        "    valueWeights = valueWeights.reshape(valueWeights.shape[0], valueWeights.shape[1], valueWeights.shape[2]*valueWeights.shape[3])\n",
        "    \n",
        "    output = self.linear(valueWeights)\n",
        "    return output\n",
        "    "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdlzpyEccMOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerUnit(nn.Module):\n",
        "  def __init__(self, embeddingSize, headsCount, dropout, factor):\n",
        "    super(TransformerUnit, self).__init__()\n",
        "    self.attention = Attention(embeddingSize, headsCount)\n",
        "    self.normalization1 = nn.LayerNorm(embeddingSize)\n",
        "    self.normalization2 = nn.LayerNorm(embeddingSize)\n",
        "\n",
        "    self.feedForward = nn.Sequential(\n",
        "        nn.Linear(embeddingSize, factor*embeddingSize),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(factor*embeddingSize, embeddingSize)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, V, K, Q, mask):\n",
        "    attention = self.attention(V, K, Q, mask)\n",
        "    feed = self.dropout(self.normalization1(attention + Q))\n",
        "    forward = self.feedForward(feed)\n",
        "    output = self.dropout(self.normalization2(forward + feed))\n",
        "    return output\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqxQpHdrTVv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocabLength, embeddingSize, layerCount, headsCount, deviceType, factor, dropout, max):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embeddingSize = embeddingSize\n",
        "    self.deviceType = deviceType\n",
        "    self.wordEmbedding = nn.Embedding(vocabLength, embeddingSize)\n",
        "    self.position = nn.Embedding(max, embeddingSize)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "         TransformerUnit(embeddingSize, headsCount, dropout, factor) for _ in range(layerCount)\n",
        "        ]\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x, mask):\n",
        "    n, seqLen = x.shape\n",
        "    pos = torch.arange(0, seqLen).expand(n, -1).to(self.deviceType)\n",
        "    output = self.dropout(self.wordEmbedding(x) + self.position(pos))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      output = layer(output, output, output, mask)\n",
        "\n",
        "    return output\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Himiv1hYdlTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderUnit(nn.Module):\n",
        "  def __init__(self, embeddingSize, headsCount, deviceType, factor, dropout):\n",
        "    super(DecoderUnit, self).__init__()\n",
        "    self.attention = Attention(embeddingSize, headsCount)\n",
        "    self.norm = nn.LayerNorm(embeddingSize)\n",
        "    self.transformer = TransformerUnit(embeddingSize, headsCount, dropout, factor)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, x, V, K, sourceMask, targetMask):\n",
        "    attention = self.attention(x, x, x, targetMask)\n",
        "    Q = self.dropout(self.norm(x + attention))\n",
        "    output = self.transformer(V, K, Q, sourceMask)\n",
        "    return output"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ReUkze1iRtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocabLength, embeddingSize, layerCount, headsCount, deviceType, factor, dropout, max):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.deviceType = deviceType\n",
        "    self.wordEmbedding = nn.Embedding(vocabLength, embeddingSize)\n",
        "    self.position = nn.Embedding(max, embeddingSize)\n",
        "    self.layers = nn.ModuleList([\n",
        "                                 DecoderUnit(embeddingSize, headsCount, deviceType, factor, dropout) for _ in range(layerCount)\n",
        "    ])\n",
        "\n",
        "    self.linear = nn.Linear(embeddingSize, vocabLength)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, encoderOutput, sourceMask, targetMask):\n",
        "    n, seqLen = x.shape\n",
        "    pos = torch.arange(0, seqLen).expand(n, -1).to(self.deviceType)\n",
        "    output = self.dropout(self.wordEmbedding(x) + self.position(pos))\n",
        "    \n",
        "    for layer in self.layers:\n",
        "      output = layer(output, encoderOutput, encoderOutput, sourceMask, targetMask)\n",
        "    \n",
        "    output = self.linear(output)\n",
        "    return output"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z-VwT2wFoKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      sourceVocab,\n",
        "      targetVocab,\n",
        "      sourcePaddingIdx,\n",
        "      targetPaddingIdx,\n",
        "      embeddingSize = 256,\n",
        "      layerCount = 4,\n",
        "      factor = 4,\n",
        "      headsCount = 4,\n",
        "      dropout = 0,\n",
        "      deviceType = \"cuda\",\n",
        "      max = 100,\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(sourceVocab, embeddingSize, layerCount, headsCount, deviceType, factor, dropout, max)\n",
        "    self.decoder = Decoder(targetVocab, embeddingSize, layerCount, headsCount, deviceType, factor, dropout, max)\n",
        "    self.sourcePaddingIdx = sourcePaddingIdx\n",
        "    self.targetPaddingIdx = targetPaddingIdx\n",
        "    self.deviceType = deviceType\n",
        "\n",
        "  def getSourceMask(self, source):\n",
        "    return (source != self.sourcePaddingIdx).unsqueeze(1).unsqueeze(2).to(self.deviceType)\n",
        "  def getTargetMask(self, target):\n",
        "    n, len = target.shape\n",
        "    return torch.tril(torch.ones(len, len)).expand(n, 1, len, len).to(self.deviceType)\n",
        "    \n",
        "  def forward(self, source, target):\n",
        "    sourceMask = self.getSourceMask(source)\n",
        "    targetMask = self.getTargetMask(target)\n",
        "    encOut = self.encoder(source, sourceMask)\n",
        "    decOut = self.decoder(target, encOut, sourceMask, targetMask)\n",
        "    return decOut"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRaO7SnIOMLS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5397d43a-4b4a-4772-effd-cde1c8a6c04f"
      },
      "source": [
        "device = torch.device(\"cuda\") #set to cpu if cuda not available\n",
        "x = torch.tensor([[1,2,3,4,5,6,7], [1,2,3,4,5, 0, 0]]).to(device)\n",
        "y = torch.tensor([[1,2,3,4,5], [5,4,3,2,1]]).to(device)\n",
        "model = Transformer(8, 8, 0, 0).to(device)\n",
        "out = model(x, y[:,:-1])\n",
        "print(out)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.4069, -0.1897,  0.0228, -1.2993, -0.4289,  0.3691,  0.0747,\n",
            "          -0.6761],\n",
            "         [ 0.3462,  0.2732, -0.8237,  0.6872,  0.2129, -0.5731,  0.5943,\n",
            "          -0.2611],\n",
            "         [ 0.2636, -0.6270, -0.4473, -0.7676, -0.4255,  0.4864,  0.2619,\n",
            "           0.0401],\n",
            "         [-0.5796, -0.6556,  0.3651, -0.7396,  0.0257, -0.9096, -0.3184,\n",
            "          -0.5247]],\n",
            "\n",
            "        [[ 0.0892,  0.0401,  0.2288, -1.1982,  0.5981, -0.9803, -0.6447,\n",
            "          -0.3452],\n",
            "         [-0.5032,  0.1254,  0.0109, -0.5198,  1.1096, -1.3209, -0.1106,\n",
            "          -0.6891],\n",
            "         [-0.0353, -0.2241, -0.2844, -1.1636, -0.0219,  0.3274, -0.0452,\n",
            "          -0.2564],\n",
            "         [-0.4210,  0.2478,  0.1842, -0.2557, -0.2561, -0.5669, -0.0486,\n",
            "          -0.5985]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWENg0_H2qmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 58,
      "outputs": []
    }
  ]
}